# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AE2Cv24CiUL8Oz-yr4wLm9B7qvFiarRY
"""

import torch
from PIL import Image
import torchvision.transforms as transforms
import random
import torch.nn as nn 
import os
import math
import cv2
import numpy as np 
from torch.nn import functional as F 
import torch.nn.init as init
from torch.hub import load_state_dict_from_url
import time
import datetime
from torch.autograd import Variable
import functools
import torchvision

torch.backends.cudnn.benchmark = True
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

save_folder = './models'
sample_folder = './samples'

if not os.path.exists(save_folder):
  os.makedirs(save_folder)
if not os.path.exists(sample_folder):
  os.makedirs(sample_folder)

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
]

def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)

def store_dataset(dir):
    images = []
    all_path = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir

    for root, _, filenames in sorted(os.walk(dir)):
        for filename in filenames:
            if is_image_file(filename):
                path = os.path.join(root, filename)
                img = Image.open(path).convert('RGB')
                images.append(img)
                all_path.append(path)
                
    return images, all_path

class BaseDataset(torch.utils.data.Dataset):
    def __init__(self):
        super(BaseDataset, self).__init__()

    def initialize(self, opt):
        pass
    
def get_transform():
    transform_list = []
    zoom = 1 + 0.1 * random.randint(0, 4)
    osize = [int(400 * zoom), int(600 * zoom)]
    transform_list.append(transforms.Resize(osize, transforms.functional.InterpolationMode.BICUBIC))
    transform_list.append(transforms.RandomCrop(256))
    transform_list.append(transforms.RandomHorizontalFlip())
    transform_list += [transforms.ToTensor(), 
                        transforms.Normalize((0.5, 0.5, 0.5),
                                                (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def __scale_width(img, target_width):
    ow, oh = img.size
    if (ow == target_width):
        return img
    w = target_width
    h = int(target_width * oh / ow)
    return img.resize((w, h), Image.BICUBIC)

class UnalignedDataset(BaseDataset):
    def initialize(self, root):
        self.root = root

        self.dir_A = os.path.join(self.root, 'trainA')
        self.dir_B = os.path.join(self.root, 'trainB')

        self.A_imgs, self.A_paths = store_dataset(self.dir_A)
        self.B_imgs, self.B_paths = store_dataset(self.dir_B)

        self.A_size = len(self.A_paths)
        self.B_size = len(self.B_paths)

        self.transform = get_transform()
    
    def __getitem__(self, index):
        A_img = self.A_imgs[index % self.A_size]
        B_img = self.B_imgs[index % self.B_size]
        A_path = self.A_paths[index % self.A_size]
        B_path = self.B_paths[index % self.B_size]

        A_img = self.transform(A_img)
        B_img = self.transform(B_img)

        w = A_img.size(2)
        h = A_img.size(1)

        if (not False) and random.random() < 0.5:
            idx = [i for i in range(A_img.size(2) -1, -1, -1)]
            idx = torch.LongTensor(idx)
            A_img = A_img.index_select(2, idx)
            B_img = B_img.index_select(2, idx)
        if (not False) and random.random() < 0.5:
            idx = [i for i in range(A_img.size(2) -1, -1, -1)]
            idx = torch.LongTensor(idx)
            A_img = A_img.index_select(1, idx)
            B_img = B_img.index_select(1, idx)
        if 1 and (not False) and random.random() < 0.5:
            times = random.randint(200, 400) / 100.
            input_img = (A_img + 1) / 2. / times
            input_img = input_img * 2 -1
        else:
            input_img = A_img
        if False:
            B_img = (B_img + 1) / 2.
            B_img = (B_img - torch.min(B_img)) / (torch.max(B_img) - torch.min(B_img))
            B_img = B_img * 2 - 1
        img = cv2.imread(A_path)
        edge_img = cv2.resize(img, (256, 256))
        A_edge = cv2.Canny(edge_img, 100, 100)
        return {'A': A_img, 'B': B_img, 'A_edge': A_edge, 'input_img': input_img, 
                'A_paths': A_path, 'B_paths': B_path}
    
    def __len__(self):
        return max(self.A_size, self.B_size)

class BaseDataLoader():
    def __init__(self):
        pass

    def initialize(self, root):
        self.root = root
        pass

    def load_data():
        return None

class UnalignedDataset2(BaseDataset):
    def initialize(self, root):
        self.root = root

        self.dir_A = os.path.join(self.root, 'trainA')
        self.dir_B = os.path.join(self.root, 'trainB')

        self.A_imgs, self.A_paths = store_dataset(self.dir_A)
        self.B_imgs, self.B_paths = store_dataset(self.dir_B)

        self.A_size = len(self.A_paths)
        self.B_size = len(self.B_paths)

        self.transform = get_transform()
    
    def __getitem__(self, index):
        A_img = self.A_imgs[index % self.A_size]
        B_img = self.B_imgs[index % self.B_size]
        A_path = self.A_paths[index % self.A_size]
        B_path = self.B_paths[index % self.B_size]

        A_img_np = np.array(A_img)
        B_img_np = np.array(B_img)

        A_img_resize = cv2.resize(A_img_np, (256, 256))
        B_img_resize = cv2.resize(B_img_np, (256, 256))
        
        A_edge = cv2.Canny(A_img_resize, 100, 100)
        A_edge = A_edge.reshape((1, ) + A_edge.shape).astype(np.float32)
        mask = self.luminosity_mask(A_img_resize)

        A_img = torch.from_numpy(A_img_resize.astype(np.float32) / 255.0).permute(2, 0, 1).contiguous()
        B_img = torch.from_numpy(B_img_resize.astype(np.float32) / 255.0).permute(2, 0, 1).contiguous()        
        mask = torch.from_numpy(mask.astype(np.float32)).contiguous()
        input_img = A_img

        return {'A': A_img, 'B': B_img, 'A_edge': A_edge, 'mask': mask, 'input_img': input_img, 
                'A_paths': A_path, 'B_paths': B_path}
    
    def lum(self, r, g, b):
      return math.sqrt(.241 * r + .691 * g + .068 * b)

    def luminosity_mask(self, img):
      pixellist = np.vstack(img).tolist()
      sorted_pixellist = sorted(pixellist, key = lambda rgb: self.lum(*rgb))
      lower_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.1)])
      upper_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.9)])
      mask = cv2.inRange(img, lower_bound, upper_bound)
      return mask.reshape((1, ) + mask.shape).astype(np.float32)

    def __len__(self):
        return max(self.A_size, self.B_size)

def CreateDataset(root):
    dataset = UnalignedDataset2()
    dataset.initialize(root)
    return dataset

class CustomDatasetDataLoader(BaseDataLoader):
    def initialize(self, root):
        BaseDataLoader.initialize(self, root)
        self.dataset = CreateDataset(root)
        self.dataloader = torch.utils.data.DataLoader(
            self.dataset,
            batch_size=4,
            shuffle=not False,
            num_workers=int(2))
            
    def load_data(self):
        return self.dataloader
    
    def __len__(self):
        return min(len(self.dataset), math.inf)

def CreateDataLoader(root):
    data_loader = CustomDatasetDataLoader()
    data_loader.initialize(root)
    return data_loader

class SpectralNorm(nn.Module):
    def __init__(self, module, name = 'weight', power_iterations = 1):
        super(SpectralNorm, self).__init__()
        self.module = module
        self.name = name
        self.power_iterations = power_iterations
        if not self._made_params():
            self._make_params()

    def _update_u_v(self):
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        w = getattr(self.module, self.name + "_bar")

        height = w.data.shape[0]
        for _ in range(self.power_iterations):
            v.data = self.l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))
            u.data = self.l2normalize(torch.mv(w.view(height,-1).data, v.data))

        sigma = u.dot(w.view(height, -1).mv(v))
        setattr(self.module, self.name, w / sigma.expand_as(w))

    def _made_params(self):
        try:
            u = getattr(self.module, self.name + "_u")
            v = getattr(self.module, self.name + "_v")
            w = getattr(self.module, self.name + "_bar")
            return True
        except AttributeError:
            return False  

    def _make_params(self):
        w = getattr(self.module, self.name)

        height = w.data.shape[0]
        width = w.view(height, -1).data.shape[1]

        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)
        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)
        u.data = self.l2normalize(u.data)
        v.data = self.l2normalize(v.data)
        w_bar = nn.Parameter(w.data)

        del self.module._parameters[self.name]

        self.module.register_parameter(self.name + "_u", u)
        self.module.register_parameter(self.name + "_v", v)
        self.module.register_parameter(self.name + "_bar", w_bar)

    def l2normalize(self, v, eps = 1e-12):
        return v / (v.norm() + eps)

    def forward(self, *args):
        self._update_u_v()
        return self.module.forward(*args)

class GatedConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):
        super(GatedConv2d, self).__init__()
        if pad_type == 'zero':
            self.pad = nn.ZeroPad2d(padding)
        else:
            assert 0, "Unsupported padding type: {}".format(pad_type)
        
        if norm == 'in':
            self.norm = nn.InstanceNorm2d(out_channels)
        elif norm == 'none':
            self.norm = None
        else:
            assert 0, "Unsupported normalization: {}".format(norm)

        if activation == 'lrelu':
            self.activation = nn.LeakyReLU(0.2, inplace = True)
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            assert 0, "Unsupported normalization: {}".format(activation)

        if sn: 
            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))
            self.mask_conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))
        else:
            self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)
            self.mask_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.pad(x)
        conv = self.conv2d(x)
        mask = self.mask_conv2d(x)
        gated_mask = self.sigmoid(mask)
        x = conv * gated_mask
        if self.norm:
            x = self.norm(x)
        if self.activation:
            x = self.activation(x)
        return x

class TransposeGatedConv2d(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = True, scale_factor = 2):
    super(TransposeGatedConv2d, self).__init__()
    self.scale_factor = scale_factor
    self.gated_conv2d = GatedConv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)
  
  def forward(self, x):
    x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')
    x = self.gated_conv2d(x)
    return x

class Conv2dLayer(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):
    super(Conv2dLayer, self).__init__()
    if pad_type == 'zero':
      self.pad = nn.ZeroPad2d(padding)
    else:
      assert 0, "Unsupported padding type: {}".format(padding)
    
    if norm == 'bn':
      self.norm = nn.BatchNorm2d(out_channels)
    elif norm == 'in':
      self.norm = nn.InstanceNorm2d(out_channels)
    elif norm == 'none':
      self.norm = None
    else:
      assert 0, "Unsupported normalization: {}".format(norm)

    if activation == 'relu':
      self.activation = nn.ReLU(inplace = True)
    elif activation =='lrelu':
      self.activation = nn.LeakyReLU(0.2, inplace = True)
    elif activation == 'none':
      self.activation = None
    else: 
      assert 0, "Unsupported activation: {}".format(activation)
    
    if sn:
      self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))
    else:
      self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)

  def forward(self, x):
    x = self.pad(x)
    x = self.conv2d(x)
    if self.norm:
      x = self.norm(x)
    if self.activation:
      x = self.activation(x)
    return x

class GatedGenerator(nn.Module):
  def __init__(self):
    super(GatedGenerator, self).__init__()
    self.coarse = nn.Sequential(
        GatedConv2d(5, 64, 7, 1, 3, pad_type = 'zero', activation = 'lrelu', norm = 'none'), 
        GatedConv2d(64, 64 * 2, 4, 2, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 2, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 4, 2, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 

        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 2, dilation = 2, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 4, dilation = 4, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 8, dilation = 8, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 16, dilation = 16, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 

        TransposeGatedConv2d(64 * 4, 64 * 2, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 2, 64 * 2, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        TransposeGatedConv2d(64 * 2, 64, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64, 3, 7, 1, 3, pad_type = 'zero', activation = 'tanh', norm = 'none'), 
    )

    self.refinement = nn.Sequential(
        GatedConv2d(5, 64, 7, 1, 3, pad_type = 'zero', activation = 'lrelu', norm = 'none'), 
        GatedConv2d(64, 64 * 2, 4, 2, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 2, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 4, 2, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 

        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 2, dilation = 2, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 4, dilation = 4, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 8, dilation = 8, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 16, dilation = 16, pad_type = 'zero', activation = 'lrelu', norm = 'in'),
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 4, 64 * 4, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 

        TransposeGatedConv2d(64 * 4, 64 * 2, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64 * 2, 64 * 2, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        TransposeGatedConv2d(64 * 2, 64, 3, 1, 1, pad_type = 'zero', activation = 'lrelu', norm = 'in'), 
        GatedConv2d(64, 3, 7, 1, 3, pad_type = 'zero', activation = 'tanh', norm = 'none'), 
    )

  def forward(self, img, mask, edges):
    print("Image Shape: {}, Mask Shape: {}, Edge Shape: {}".format(img.shape, mask.shape, edges.shape))
    first_masked_img = img * (1 - mask) + mask
    first_in = torch.cat((first_masked_img, mask, edges), 1)
    first_out = self.coarse(first_in)

    second_masked_img = img * (1 - mask) + first_out * mask
    second_in = torch.cat((second_masked_img, mask, edges), 1)
    second_out = self.refinement(second_in)    
    return first_out, second_out

class Discriminator(nn.Module):
  def __init__(self):
      super(Discriminator, self).__init__()

      def discriminator_block(in_filters, out_filters, bn=True):
          block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]
          if bn:
              block.append(nn.BatchNorm2d(out_filters, 0.8))
          return block

      self.model = nn.Sequential(
          *discriminator_block(3, 16, bn=False),
          *discriminator_block(16, 32),
          *discriminator_block(32, 64),
          *discriminator_block(64, 128),
      )

      ds_size = 256 // 2 ** 4
      self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))

  def forward(self, img):
      out = self.model(img)
      out = out.view(out.shape[0], -1)
      validity = self.adv_layer(out)

      return validity

class NLayerDiscriminator(nn.Module):

  def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):
      
    super(NLayerDiscriminator, self).__init__()
    if type(norm_layer) == functools.partial: 
      use_bias = norm_layer.func == nn.InstanceNorm2d
    else:
      use_bias = norm_layer == nn.InstanceNorm2d

    kw = 4
    padw = 1
    sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]
    nf_mult = 1
    nf_mult_prev = 1
    for n in range(1, n_layers):  
      nf_mult_prev = nf_mult
      nf_mult = min(2 ** n, 8)
      sequence += [
        nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),
        norm_layer(ndf * nf_mult),
        nn.LeakyReLU(0.2, True)
      ]

    nf_mult_prev = nf_mult
    nf_mult = min(2 ** n_layers, 8)
    sequence += [
      nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),
      norm_layer(ndf * nf_mult),
      nn.LeakyReLU(0.2, True)
    ]

    sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  
    self.model = nn.Sequential(*sequence)

  def forward(self, input):
      return self.model(input)

cfgs = {
  'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
  'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
  'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
  'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}

model_urls = {
  'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',
  'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',
  'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',
  'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',
  'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',
  'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',
  'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',
  'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',
}

def make_layers(cfg, batch_norm=False):
  layers = []
  in_channels = 3
  for v in cfg:
    if v == 'M':
      layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
    else:
      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
      if batch_norm:
        layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
      else:
        layers += [conv2d, nn.ReLU(inplace=True)]
      in_channels = v
  return nn.Sequential(*layers)

class VGG(nn.Module):

  def __init__(self, features, num_classes=1000, init_weights=True):
    super(VGG, self).__init__()
    self.features = features
    self.classifier = nn.Sequential(
      nn.Linear(512 * 7 * 7, 4096),
      nn.ReLU(True),
      nn.Dropout(),
      nn.Linear(4096, 4096),
      nn.ReLU(True),
      nn.Dropout(),
      nn.Linear(4096, num_classes),
    )
    if init_weights:
      self._initialize_weights()

  def forward(self, x):
    x = self.features(x)
    return x

  def _initialize_weights(self):
    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
          nn.init.constant_(m.bias, 0)
      elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
      elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.constant_(m.bias, 0)

def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):
  if pretrained:
    kwargs['init_weights'] = False
  model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)
  if pretrained:
    state_dict = load_state_dict_from_url(model_urls[arch],
                                          progress=progress)
    model.load_state_dict(state_dict)
  return model

def vgg16_bn(pretrained=False, progress=True, **kwargs):
  return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)

data_loader = CreateDataLoader('/content/drive/MyDrive/final_dataset')

dataset = data_loader.load_data()
dataset_size = len(data_loader)
print('Number of training images = %d' % dataset_size)
print(f'Shape of image = {next(iter(dataset))["A"].shape}')

def weights_init(network, init_type = 'kaiming', init_gain = 0.02):
  def init_func(m):
    classname = m.__class__.__name__
    if hasattr(m, 'weight') and classname.find('Conv') != -1:
      if init_type == 'normal':
          init.normal_(m.weight.data, 0.0, init_gain)
      elif init_type == 'xavier':
          init.xavier_normal_(m.weight.data, gain = init_gain)
      elif init_type == 'kaiming':
          init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')
      elif init_type == 'orthogonal':
          init.orthogonal_(m.weight.data, gain = init_gain)
      else:
          raise NotImplementedError('inititalization method [%s] is not implemented' % init_type)
    elif classname.find('BatchNorm2d') != -1:
      init.normal_(m.weight.data, 1.0, 0.02)
      init.constant_(m.bias.data, 0.0)
    elif classname.find('Linear') != -1:
      init.normal_(m.weight, 0, 0.01)
      init.constant_(m.bias, 0)
  
  network.apply(init_func)

def adjust_learning_rate(lr_in, optimizer, epoch):
  lr = lr_in + (10 ** (epoch // 10))
  for param_group in optimizer.param_groups:
    param_group['lr'] = lr

def save_model(net, epoch, save_folder):
  model_name = 'LSGAN_epoch%d_batchsize%d.pth' % (epoch, 1)
  model_name = os.path.join(save_folder, model_name)
  if epoch % 1 == 0:
    torch.save(net.state_dict(), model_name)
    print('The trained model is successfully saved at epoch %d' % (epoch))

def save_sample_png(sample_folder, sample_name, img_list, name_list, pixel_max_cnt = 255):
  for i in range(len(img_list)):
    img = img_list[i]
    img = img * 255
    img_copy = img.clone().data.permute(0, 2, 3, 1)[0, :, :, :].cpu().numpy()
    img_copy = np.clip(img_copy, 0, pixel_max_cnt)
    img_copy = img_copy.astype(np.uint8)
    save_img_name = sample_name + '_' + name_list[i] + '.png'
    save_img_path = os.path.join(sample_folder, save_img_name)
    cv2.imwrite(save_img_path, img_copy)

init_type = 'xavier'
generator = GatedGenerator()
print('Generator is created')
weights_init(generator, init_type = init_type, init_gain = 0.02)
print('Initialize generator with %s type' % init_type)

discriminator = Discriminator()
print('Discriminator is generated')
weights_init(discriminator, init_type = init_type, init_gain = 0.02)
print('Initialize discriminator with %s type' % init_type)

perceptualnet = vgg16_bn(pretrained=True)
print('Perceptual network is created')

generator = generator.to(device)
discriminator = discriminator.to(device)
perceptualnet = perceptualnet.to(device)

L1Loss = nn.L1Loss().to(device)
adversarial_loss = nn.BCEWithLogitsLoss().to(device)

optimizer_g = torch.optim.Adam(generator.parameters(), lr = 1e-4, betas = (0.5, 0.999), weight_decay = 0)
optimizer_d = torch.optim.Adam(discriminator.parameters(), lr = 4e-4, betas = (0.5, 0.999), weight_decay = 0)

dataset = data_loader.load_data()

def save_sample_png(sample_folder, sample_name, img_list, name_list):
  torchvision.utils.save_image(img_list[0], sample_name + '_' + name_list[0] + '.png')
  torchvision.utils.save_image(img_list[1], sample_name + '_' + name_list[1] + '.png')
  torchvision.utils.save_image(img_list[2], sample_name + '_' + name_list[2] + '.png')
  torchvision.utils.save_image(img_list[3], sample_name + '_' + name_list[3] + '.png')
  torchvision.utils.save_image(img_list[4], sample_name + '_' + name_list[4] + '.png')

Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor

G_losses = []
D_losses = []

prev_time = time.time()
for epoch in range(40):
  for batch_idx, data in enumerate(dataset):
    A_img = data['A'].to(device)
    B_img = data['B'].to(device)
    A_edge = data['A_edge'].to(device)
    mask = data['mask'].to(device)
    input_img = data['input_img'].to(device)
    image_paths = data['A_paths']

    optimizer_d.zero_grad()

    first_out, second_out = generator(A_img, mask, A_edge)
    first_out_wholeimg = A_img * (1 - mask) + first_out * mask
    second_out_wholeimg = A_img * (1 - mask) + second_out * mask

    valid = Variable(Tensor(B_img.shape[0], 1).fill_(1.0), requires_grad=False)
    fake = Variable(Tensor(B_img.shape[0], 1).fill_(0.0), requires_grad=False)

    real_pred = discriminator(B_img)
    fake_pred = discriminator(second_out_wholeimg.detach())

    real_loss = adversarial_loss(real_pred - fake_pred.mean(0, keepdim=True), valid)
    fake_loss = adversarial_loss(fake_pred - real_pred.mean(0, keepdim=True), fake)
    loss_D = (real_loss + fake_loss) / 2
    loss_D.backward()
    optimizer_d.step()

    optimizer_g.zero_grad()
    
    first_MaskL1Loss = L1Loss(first_out_wholeimg.detach(), A_img)
    second_MaskL1Loss = L1Loss(second_out_wholeimg.detach(), A_img)

    img_featuremaps = perceptualnet(A_img)                            
    second_out_wholeimg_featuremaps = perceptualnet(second_out_wholeimg.detach())
    second_PerceptualLoss = L1Loss(second_out_wholeimg_featuremaps, img_featuremaps)

    loss_G = 10 * first_MaskL1Loss + 10 * second_MaskL1Loss + second_PerceptualLoss
    loss_G.backward()
    optimizer_g.step()

    batches_done = epoch * len(data_loader) + batch_idx
    batches_left = 40 * len(data_loader) - batches_done
    time_left = datetime.timedelta(seconds = batches_left * (time.time() - prev_time))
    prev_time = time.time()

    print("\r[Epoch %d/%d] [Batch %d/%d] [first Mask L1 Loss: %.5f] [second Mask L1 Loss: %.5f]" %
        ((epoch + 1), 40, batch_idx, len(data_loader), first_MaskL1Loss.item(), second_MaskL1Loss.item()))
    print("\r[D Loss: %.5f] [G Loss: %.5f] [Perceptual Loss: %.5f] time_left: %s" %
        (loss_D.item(), loss_G.item(), second_PerceptualLoss.item(), time_left))
    
    G_losses.append(loss_G.item())
    D_losses.append(loss_D.item())
    
  adjust_learning_rate(1e-4, optimizer_g, (epoch + 1))
  adjust_learning_rate(4e-4, optimizer_d, (epoch + 1))
  save_model(generator, (epoch + 1), './models')
  masked_img = A_img * (1 - mask) + mask
  mask = torch.cat((mask, mask, mask), 1)

  if (epoch + 1) % 1 == 0:
    img_list = [A_img, mask, masked_img, first_out, second_out]
    name_list = ['gt', 'mask', 'masked_img', 'first_out', 'second_out']
    save_sample_png(sample_folder = sample_folder, sample_name = 'epoch%d' % (epoch + 1), img_list = img_list, name_list = name_list)

import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

def lum(r, g, b):
  return math.sqrt(.241 * r + .691 * g + .068 * b)

def luminosity_mask(img):
  pixellist = np.vstack(img).tolist()
  sorted_pixellist = sorted(pixellist, key = lambda rgb: lum(*rgb))
  lower_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.1)])
  upper_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.9)])
  mask = cv2.inRange(img, lower_bound, upper_bound)
  return mask.reshape((1, ) + mask.shape).astype(np.float32)

test_img = Image.open('/content/drive/MyDrive/final_dataset/testA/low00039.png')
test_img_np = np.array(test_img)
test_img_resize = cv2.resize(test_img_np, (256, 256))
test_edge = cv2.Canny(test_img_resize, 100, 100)
r_test_edge = test_edge.reshape((1, ) + test_edge.shape).astype(np.float32)
r_test_edge = torch.from_numpy(r_test_edge).contiguous()
pixellist = np.vstack(test_img_resize).tolist()
sorted_pixellist = sorted(pixellist, key = lambda rgb: lum(*rgb))
lower_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.1)])
upper_bound = np.array(sorted_pixellist[int(len(sorted_pixellist) * 0.9)])
mask = cv2.inRange(test_img_resize, lower_bound, upper_bound)
r_mask = mask.reshape((1, ) + mask.shape).astype(np.float32)
r_mask = torch.from_numpy(r_mask.astype(np.float32)).contiguous()
test_img_resize = torch.from_numpy(test_img_resize.astype(np.float32) / 255.0).permute(2, 0, 1).contiguous()
test_img_resize = np.reshape(test_img_resize, (1,) + test_img_resize.shape)
r_mask = np.reshape(r_mask, (1,) + r_mask.shape)
r_test_edge = np.reshape(r_test_edge, (1,) + r_test_edge.shape)
_, second_out = generator(test_img_resize.to(device), r_mask.to(device), r_test_edge.to(device))